---
title: "PPOL670 FINAL PROJECT"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Packages

```{r}
require(tidyverse) #loads the package tidyverse
require(caret) #loads the package caret
require(lubridate) #loads the package lubridate
require(ggplot2) #loads the package ggplot2
require(recipes) #loads the package recipes
require(rattle) # For nice tree plots
require(ranger) # loads the package ranger
require(skimr) # loads the package skimr
require(ggthemes) #loads the package ggthemes
require(mice) #loads the package mice
require(fastDummies) #For quickly creating dummy variables
require(dendroTools) #For quickly rounding numbers
```


## Importing Data 


```{r}

sample_adult_raw = read_csv("Raw Data/samadult.csv") #imports data into the R memory 
person_raw = read_csv("Raw Data/personsx.csv") #imports data into the R memory 


```

## Creating dataset

```{r}
#creating datasets by selecting variables of interest from raw data

dataset_adult = sample_adult_raw %>% select(AFLHCA17,RECTYPE,REGION,SEX,HISPAN_I,RACERPI2,AGE_P,R_MARITL,DOINGLWA,HYPYR1,CHLYR,MIEV,STREV,COPDEV,CANEV,PREGEVER,DIBEV1,EPILEP1,SMKNOW,ALC12MWK,AHCAFYR2,AHCSYR1,AINDPRCH,MBO_MAN1,MBO_MND1,YTQU_YG1,AHEIGHT,AWEIGHTP)

dataset_person = person_raw %>% select(RECTYPE,ERNYR_P,PLBORN,PHSTAT)

```


## Merging the datasets

```{r}
final_dataset_raw = full_join(dataset_person,dataset_adult,by="RECTYPE")
sapply(colnames(final_dataset_raw), function(x) class(final_dataset_raw[[x]])) #Gives us the class of each variable

```

## Dropping RECTYPE
```{r}
#Dropiing Variable RECTYPE as it is an record identifier and was used to merge datasets
final_dataset_raw=final_dataset_raw  %>% select(-RECTYPE)
```

## Recoding Dependent Variable

```{r}
final_dataset_raw=mutate(final_dataset_raw,
       occurrence = AFLHCA17 == '1',
       occurrence = as.numeric(occurrence)) #Creating a new variable 'occurrence' based on AFLHCA17
final_dataset_raw=final_dataset_raw  %>% select(-AFLHCA17) #Dropping the variable AFLHCA17 as we longer need it

```



## Taking a Random Sample from Data

```{r}
final_dataset=final_dataset_raw %>% sample_n(10000) # Only taking a random sample of the data so the models run quicker
head(final_dataset) # Peek at the data just to make sure everything was read in correctly. 


```

## Partioning the dataset

```{r}
## 75% of the sample size
smp_size <- floor(0.75 * nrow(final_dataset))

## Creating partion
set.seed(123)
train_ind <- sample(seq_len(nrow(final_dataset)), size = smp_size)

train_data <- final_dataset[train_ind, ] #creates training dataset
test_data <- final_dataset[-train_ind, ] #creates test dataset
```

## Summarizing the variables in the training dataset

```{r}
skimr::skim(train_data) #Gives a summary statics of all the variables in train_data
```

## Graphing histogram for Age variable
 
```{r,}
# Creating a density graph for Age
train_data %>% 
  select(AGE_P) %>% 
  gather(var,val) %>% 
  ggplot(aes(val,group=var)) +
  geom_density(bins = 68, fill = "azure") + ggtitle("Distribution of observations by Age")+ # for the main title
xlab("Age")+ # for the x axis label
ylab("Frequency")+# for the y axis label
  theme_tufte()


```
## Graphing histogram for Height variable
 
```{r,}
# Creating a histogram for Height
train_data %>% 
  select(AHEIGHT) %>% 
  gather(var,val) %>% 
  ggplot(aes(val,group=var)) +
  geom_histogram(bins =30, fill = "darkolivegreen") + ggtitle("Distribution of observations by Height") + # for the main title
xlab("Height")+ # for the x axis label
ylab("Frequency")+ # for the y axis label  
  theme_stata()


```

## Graphing histogram for Weight variable
 
```{r,}
# Creating a histogram for Weight
train_data %>% 
  select(AWEIGHTP) %>% 
  gather(var,val) %>% 
  ggplot(aes(val,group=var)) +
  geom_histogram(bins =30, fill = "cornflowerblue") + ggtitle("Distribution of observations by Weight")+ # for the main title
xlab("Weight")+ # for the x axis label
ylab("Frequency")+# for the y axis label
  theme_stata()


```


## Graphing categorical variables
```{r}
# Creating a bar graph for Region variable
train_data %>% 
  mutate(REGION_bins = case_when(
    REGION == 1 ~ "Northeast",
    REGION == 2 ~ "Midwest",
    REGION == 3 ~ "South",
    REGION == 4 ~ "West",
  )) %>% 
  mutate(REGION_bins = fct_infreq(REGION_bins)) %>% 
  ggplot(aes(REGION_bins)) + geom_bar(fill="steelblue") + ggtitle("Distribution of observations by Region")+ # for the main title
xlab("Region")+ # for the x axis label
ylab("Frequency")+
  theme_economist()
  
```

## Graphing categorical variables
```{r}
# Creating a bar graph for Sex variable
train_data %>% 
  mutate(SEX_bins = case_when(
    SEX == 1 ~ "Male",
    SEX == 2 ~ "Female",
    
  )) %>% 
  mutate(SEX_bins = fct_infreq(SEX_bins)) %>% 
  ggplot(aes(SEX_bins)) + geom_bar(fill="darkred") + ggtitle("Distribution of observations by Sex")+ # for the main title
xlab("Sex")+ # for the x axis label
ylab("Frequency")+
  theme_economist()
  
```

## Graphing categorical variables
```{r}
# Creating a bar graph for Race Variable
train_data %>% 
  mutate(RACERPI2_bins = case_when(
    RACERPI2 == 1 ~ "White",
    RACERPI2 == 2 ~ "African American",
    RACERPI2 == 3 ~ "AIAN",
    RACERPI2 == 4 ~ "Asian",
    RACERPI2 == 5 ~ "Unknown",
    RACERPI2 == 6 ~ "Multiple Race",
    
  )) %>% 
  mutate(RACERPI2_bins = fct_infreq(RACERPI2_bins)) %>% 
  ggplot(aes(RACERPI2_bins)) + geom_bar(fill="darkolivegreen") + ggtitle("Distribution of observations by Race")+ # for the main title
xlab("Race")+ # for the x axis label
ylab("Frequency")+
  theme_economist()
  
```

## Graphing categorical variables
```{r}
# Creating a bar graph for Earnings
train_data %>% 
  mutate(ERNYR_P_bins = case_when(
    ERNYR_P == 01 ~ "0-5k",
    ERNYR_P == 02 ~ "5k-10k",
    ERNYR_P == 03 ~ "10k-15k",
ERNYR_P == 04 ~ "15k-20k",
ERNYR_P == 05 ~ "20k-25k",
ERNYR_P == 06 ~ "25k-35k",
ERNYR_P == 07 ~ "35k-45k",
ERNYR_P == 08 ~ "45k-55k",
ERNYR_P == 09 ~ "55k-65k",
ERNYR_P == 10 ~ "65k-75k",
ERNYR_P == 11 ~ "75k+",
ERNYR_P == 97 ~ "Refused",
ERNYR_P == 98 ~ "Not Certain",
ERNYR_P == 99 ~ "Don't Know"
,
    
    
    
    
  )) %>% 
  mutate(ERNYR_P_bins = fct_infreq(ERNYR_P_bins)) %>% 
  ggplot(aes(ERNYR_P_bins)) + geom_bar(fill="chocolate4") + ggtitle("Distribution of observations by Earnings")+ # for the main title
xlab("Total Earnings")+ # for the x axis label
ylab("Frequency")+
  theme_economist() +coord_flip()
  
```

##Pre-Processing Data

### Setting Categorical variables as factor variable
```{r}
#Changing class of categorical variables to factor
train_data=train_data %>% mutate(ERNYR_P = as.factor(ERNYR_P),
                             PLBORN = as.factor(PLBORN),
                             PHSTAT = as.factor(PHSTAT),
                             REGION = as.factor(REGION),
                             SEX = as.factor(SEX),
                             HISPAN_I = as.factor(HISPAN_I),
                             RACERPI2 = as.factor(RACERPI2),
                             R_MARITL = as.factor(R_MARITL),
                             DOINGLWA = as.factor(DOINGLWA),
                             HYPYR1 = as.factor(HYPYR1),
                             CHLYR = as.factor(CHLYR),
                             MIEV = as.factor(MIEV),
                             STREV = as.factor(STREV),
                             COPDEV = as.factor(COPDEV),
                             CANEV = as.factor(CANEV),
                             PREGEVER = as.factor(PREGEVER),
                             DIBEV1 = as.factor(DIBEV1),
                             EPILEP1 = as.factor(EPILEP1),
                             SMKNOW = as.factor(SMKNOW),
                             ALC12MWK = as.factor(ALC12MWK),
                             AHCAFYR2 = as.factor(AHCAFYR2),
                             AHCSYR1 = as.factor(AHCSYR1),
                             AINDPRCH = as.factor(AINDPRCH),
                             MBO_MAN1 = as.factor(MBO_MAN1),
                             MBO_MND1 = as.factor(MBO_MND1),
                             YTQU_YG1 = as.factor(YTQU_YG1),
                             occurrence  = as.factor(occurrence)
                             
)


```

### Creating recipe to impute missing data 


```{r}
# Generating a recipe to impute missing data 

rcp1 <- recipe(occurrence~.,train_data) %>% step_modeimpute(all_nominal()) %>% step_meanimpute(all_numeric()) %>% step_range(all_numeric()) %>% prep() 

rcp1

# Apply the recipe to the training and test data

train_data1 <- bake(rcp1,train_data)

test_data1= bake(rcp1,test_data)

# Converting factor variables to dummy variable for each level

train_data2 <- fastDummies::dummy_cols(train_data1, remove_first_dummy = TRUE)
test_data2 <- fastDummies::dummy_cols(test_data1, remove_first_dummy = TRUE)

#Dropping Factor variables except for the dependent variable

train_data2= train_data2 %>% select(-REGION,-SEX,-HISPAN_I,-RACERPI2,-R_MARITL,-DOINGLWA,-HYPYR1,-CHLYR,-MIEV,-STREV,-COPDEV,-CANEV,-PREGEVER,-DIBEV1,-EPILEP1,-SMKNOW,-ALC12MWK,-AHCAFYR2,-AHCSYR1,-AINDPRCH,-MBO_MAN1,-MBO_MND1,-YTQU_YG1,-ERNYR_P,-PLBORN,-PHSTAT)

test_data2= test_data2 %>% select(-REGION,-SEX,-HISPAN_I,-RACERPI2,-R_MARITL,-DOINGLWA,-HYPYR1,-CHLYR,-MIEV,-STREV,-COPDEV,-CANEV,-PREGEVER,-DIBEV1,-EPILEP1,-SMKNOW,-ALC12MWK,-AHCAFYR2,-AHCSYR1,-AINDPRCH,-MBO_MAN1,-MBO_MND1,-YTQU_YG1,-ERNYR_P,-PLBORN,-PHSTAT)
```



## Glimpsing data

```{r}
glimpse(train_data2) #gives a glimpse of the dataset train_data2
glimpse(test_data2) #gives a glimpse of the dataset test_data2

```


## Creating Folds

```{r}
set.seed(1988) # set a seed for replication purposes 


folds <- createFolds(train_data2$occurence , k = 5) # Partition the data into 5 equal folds

sapply(folds,length) #displays the folds with their length. 

control_conditions <- 
  trainControl(method='cv', # K-fold cross validation
               index = folds # The indices for our folds (so they are always the same)
  )
```

## Running a Linear Regression on AFLHCA17  with all other variables in train_data2

```{r}
mod_lm <-
  train(occurence  ~ .,          # Equation (outcome and everything else)
        data=train_data2, # Training data 
        method = "lm",    # linear model
        metric = "RMSE",   # mean squared error
        trControl = control_conditions # Cross validation conditions
  )
```

### Printing the results of the linear model
```{r}
mod_lm #prints the result of the linear model
```
  
## K-Nearest Neighbors with tuning parameters k=1,5,10, and 50

```{r}

knn_tune = expand.grid(k = c(1,5,10,50))

mod_knn <-
  train(occurence  ~ .,           # Equation (outcome and everything else)
        data=train_data2,  # Training data 
        method = "knn",    # K-Nearest Neighbors Algorithm
        metric = "RMSE",   # mean squared error
        trControl = control_conditions, # Cross validation conditions
        tuneGrid = knn_tune # vary tuning parameter K
  )
```
  
### Printing the results of the KNN model  
```{r}
mod_knn #Prints the results of the KNN model
```

### Plotting the KNN model

```{r}
plot(mod_knn) #plots the knn model
```

## Classification and Regression Trees (CART)

```{r}
mod_cart <-
   train(occurence ~ .,            # Equation (outcome and everything else)
        data=train_data2,    # Training data 
        method = "rpart",    # Regression tree
        metric = "RMSE",     # mean squared error
        trControl = control_conditions # Cross validation conditions
  )
```

### Printing the results of the CART 

```{r}
mod_cart #prints the results of the CART 
```

### Graphing the CART

```{r}
plot(mod_cart) #Graphs the CART
```


### Visualizing the Tree

```{r}
fancyRpartPlot(mod_cart$finalModel) #Plots the Tree
```

### Let's try to model a tree with depth 0.006

```{r}
tune_cart <- expand.grid(cp = c(0.006)) # Complexity Parameter (how "deep" our trees should grow)
mod_cart2 <-
  train(occurence ~ ., # Equation (outcome and everything else)
        data=train_data2, # Training data 
        method = "rpart", # Classification Tree
        metric = "RMSE",     # mean squared error
        tuneGrid = tune_cart, # Tuning parameters
        trControl = control_conditions  )
```

### Printing the results of the tuned Tree

```{r}

print(tune_cart)

```

### Random Forest

```{r}

tunegrid <- expand.grid(mtry=c(2,4,6), splitrule=c("variance", "extratrees"), min.node.size=c(5))

mod_rf <-
  train(occurence ~ ., # Equation (outcome and everything else)
        data=train_data2, # Training data 
        method = "ranger", # random forest (ranger is much faster than rf)
        metric = "RMSE",     # mean squared error
        trControl = control_conditions,
        tuneGrid=tunegrid
  )
```
1


### Printing the Random Forest Model
```{r}
mod_rf
```

### Ploting the Random Forest Model

```{r}
plot(mod_rf)
```

## Summarizing all the models 

```{r}
# Organize all model imputs as a list.
mod_list <-
  list(
    lm = mod_lm,
    
    cart = mod_cart,
    cart_deep = mod_cart2,
    rf = mod_rf 
  )

# Resamples allows us to compare model output
resamples(mod_list)
```

### Examine the error across each of the five models.
```{r,fig.width=10,fig.height=4}
dotplot(resamples(mod_list),metric = "RMSE")
```

### Examine the fit across each of the five models.
```{r,fig.width=10,fig.height=4}
dotplot(resamples(mod_list),metric = "Rsquared") #plots a dot plot with all five models
```

## Testing the Predictive Accuracy of the Random Forest Model 


```{r}
pred <- predict(mod_rf,newdata = test_data2)
mse = sum(test_data2$occurence-pred^2)/nrow(test_data2) #calculates object 'mse'
mse 
``` 
