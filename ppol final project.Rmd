---
title: "PPOL670 FINAL PROJECT"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Packages

```{r}
require(tidyverse) #loads the package tidyverse
require(caret) #loads the package caret
require(lubridate) #loads the package lubridate
require(ggplot2) #loads the package ggplot2
require(recipes) #loads the package recipes
require(rattle) # For nice tree plots
require(ranger) # loads the package ranger
require(skimr) # loads the package skimr
require(ggthemes) #loads the package ggthemes
require(mice) #loads the package mice
require(fastDummies) #For quickly creating dummy variables
require(dendroTools) #For quickly rounding numbers
require(kernlab) #For Running SVM models
```


## Importing Data 


```{r}

sample_adult_raw = read_csv("Raw Data/samadult.csv") #imports data into the R memory 
person_raw = read_csv("Raw Data/personsx.csv") #imports data into the R memory 


```

## Creating dataset

```{r}
#creating datasets by selecting variables of interest from raw data

dataset_adult = sample_adult_raw %>% select(AFLHCA17,RECTYPE,REGION,SEX,HISPAN_I,RACERPI2,AGE_P,R_MARITL,DOINGLWA,HYPYR1,CHLYR,MIEV,STREV,COPDEV,CANEV,PREGEVER,DIBEV1,EPILEP1,SMKNOW,ALC12MWK,AHCAFYR2,AHCSYR1,AINDPRCH,MBO_MAN1,MBO_MND1,YTQU_YG1,AHEIGHT,AWEIGHTP)

dataset_person = person_raw %>% select(RECTYPE,ERNYR_P,PLBORN,PHSTAT)

```


## Merging the datasets

```{r}
final_dataset_raw = full_join(dataset_person,dataset_adult,by="RECTYPE")
sapply(colnames(final_dataset_raw), function(x) class(final_dataset_raw[[x]])) #Gives us the class of each variable

```

## Dropping RECTYPE
```{r}
#Dropiing Variable RECTYPE as it is an record identifier and was used to merge datasets
final_dataset_raw=final_dataset_raw  %>% select(-RECTYPE)
```

## Recoding Dependent Variable

```{r}
final_dataset_raw=mutate(final_dataset_raw,
       occurrence = AFLHCA17 == '1',
       occurrence = as.numeric(occurrence)) #Creating a new variable 'occurrence' based on AFLHCA17
final_dataset_raw=final_dataset_raw  %>% select(-AFLHCA17) #Dropping the variable AFLHCA17 as we longer need it

```



## Taking a Random Sample from Data

```{r}
final_dataset=final_dataset_raw %>% sample_n(10000) # Only taking a random sample of the data so the models run quicker
head(final_dataset) # Peek at the data just to make sure everything was read in correctly. 


```

## Partioning the dataset

```{r}
## 75% of the sample size
smp_size <- floor(0.75 * nrow(final_dataset))

## Creating partion
set.seed(123)
train_ind <- sample(seq_len(nrow(final_dataset)), size = smp_size)

train_data <- final_dataset[train_ind, ] #creates training dataset
test_data <- final_dataset[-train_ind, ] #creates test dataset
```

## Summarizing the variables in the training dataset

```{r}
skimr::skim(train_data) #Gives a summary statics of all the variables in train_data
```

## Graphing histogram for Age variable
 
```{r,}
# Creating a density graph for Age
train_data %>% 
  select(AGE_P) %>% 
  gather(var,val) %>% 
  ggplot(aes(val,group=var)) +
  geom_density(bins = 68, fill = "azure") + ggtitle("Distribution of observations by Age")+ # for the main title
xlab("Age")+ # for the x axis label
ylab("Frequency")+# for the y axis label
  theme_tufte()


```
## Graphing histogram for Height variable
 
```{r,}
# Creating a histogram for Height
train_data %>% 
  select(AHEIGHT) %>% 
  gather(var,val) %>% 
  ggplot(aes(val,group=var)) +
  geom_histogram(bins =30, fill = "darkolivegreen") + ggtitle("Distribution of observations by Height") + # for the main title
xlab("Height")+ # for the x axis label
ylab("Frequency")+ # for the y axis label  
  theme_stata()


```

## Graphing histogram for Weight variable
 
```{r,}
# Creating a histogram for Weight
train_data %>% 
  select(AWEIGHTP) %>% 
  gather(var,val) %>% 
  ggplot(aes(val,group=var)) +
  geom_histogram(bins =30, fill = "cornflowerblue") + ggtitle("Distribution of observations by Weight")+ # for the main title
xlab("Weight")+ # for the x axis label
ylab("Frequency")+# for the y axis label
  theme_stata()


```


## Graphing categorical variables
```{r}
# Creating a bar graph for Region variable
train_data %>% 
  mutate(REGION_bins = case_when(
    REGION == 1 ~ "Northeast",
    REGION == 2 ~ "Midwest",
    REGION == 3 ~ "South",
    REGION == 4 ~ "West",
  )) %>% 
  mutate(REGION_bins = fct_infreq(REGION_bins)) %>% 
  ggplot(aes(REGION_bins)) + geom_bar(fill="steelblue") + ggtitle("Distribution of observations by Region")+ # for the main title
xlab("Region")+ # for the x axis label
ylab("Frequency")+
  theme_economist()
  
```

## Graphing categorical variables
```{r}
# Creating a bar graph for Sex variable
train_data %>% 
  mutate(SEX_bins = case_when(
    SEX == 1 ~ "Male",
    SEX == 2 ~ "Female",
    
  )) %>% 
  mutate(SEX_bins = fct_infreq(SEX_bins)) %>% 
  ggplot(aes(SEX_bins)) + geom_bar(fill="darkred") + ggtitle("Distribution of observations by Sex")+ # for the main title
xlab("Sex")+ # for the x axis label
ylab("Frequency")+
  theme_economist()
  
```

## Graphing categorical variables
```{r}
# Creating a bar graph for Race Variable
train_data %>% 
  mutate(RACERPI2_bins = case_when(
    RACERPI2 == 1 ~ "White",
    RACERPI2 == 2 ~ "African American",
    RACERPI2 == 3 ~ "AIAN",
    RACERPI2 == 4 ~ "Asian",
    RACERPI2 == 5 ~ "Unknown",
    RACERPI2 == 6 ~ "Multiple Race",
    
  )) %>% 
  mutate(RACERPI2_bins = fct_infreq(RACERPI2_bins)) %>% 
  ggplot(aes(RACERPI2_bins)) + geom_bar(fill="darkolivegreen") + ggtitle("Distribution of observations by Race")+ # for the main title
xlab("Race")+ # for the x axis label
ylab("Frequency")+
  theme_economist()
  
```

## Graphing categorical variables
```{r}
# Creating a bar graph for Earnings
train_data %>% 
  mutate(ERNYR_P_bins = case_when(
    ERNYR_P == 01 ~ "0-5k",
    ERNYR_P == 02 ~ "5k-10k",
    ERNYR_P == 03 ~ "10k-15k",
ERNYR_P == 04 ~ "15k-20k",
ERNYR_P == 05 ~ "20k-25k",
ERNYR_P == 06 ~ "25k-35k",
ERNYR_P == 07 ~ "35k-45k",
ERNYR_P == 08 ~ "45k-55k",
ERNYR_P == 09 ~ "55k-65k",
ERNYR_P == 10 ~ "65k-75k",
ERNYR_P == 11 ~ "75k+",
ERNYR_P == 97 ~ "Refused",
ERNYR_P == 98 ~ "Not Certain",
ERNYR_P == 99 ~ "Don't Know"
,
    
    
    
    
  )) %>% 
  mutate(ERNYR_P_bins = fct_infreq(ERNYR_P_bins)) %>% 
  ggplot(aes(ERNYR_P_bins)) + geom_bar(fill="chocolate4") + ggtitle("Distribution of observations by Earnings")+ # for the main title
xlab("Total Earnings")+ # for the x axis label
ylab("Frequency")+
  theme_economist() +coord_flip()
  
```

##Pre-Processing Data

### Setting Categorical variables as factor variable
```{r}
#Changing class of categorical variables to factor
train_data=train_data %>% mutate(ERNYR_P = as.factor(ERNYR_P),
                             PLBORN = as.factor(PLBORN),
                             PHSTAT = as.factor(PHSTAT),
                             REGION = as.factor(REGION),
                             SEX = as.factor(SEX),
                             HISPAN_I = as.factor(HISPAN_I),
                             RACERPI2 = as.factor(RACERPI2),
                             R_MARITL = as.factor(R_MARITL),
                             DOINGLWA = as.factor(DOINGLWA),
                             HYPYR1 = as.factor(HYPYR1),
                             CHLYR = as.factor(CHLYR),
                             MIEV = as.factor(MIEV),
                             STREV = as.factor(STREV),
                             COPDEV = as.factor(COPDEV),
                             CANEV = as.factor(CANEV),
                             PREGEVER = as.factor(PREGEVER),
                             DIBEV1 = as.factor(DIBEV1),
                             EPILEP1 = as.factor(EPILEP1),
                             SMKNOW = as.factor(SMKNOW),
                             ALC12MWK = as.factor(ALC12MWK),
                             AHCAFYR2 = as.factor(AHCAFYR2),
                             AHCSYR1 = as.factor(AHCSYR1),
                             AINDPRCH = as.factor(AINDPRCH),
                             MBO_MAN1 = as.factor(MBO_MAN1),
                             MBO_MND1 = as.factor(MBO_MND1),
                             YTQU_YG1 = as.factor(YTQU_YG1),
                             occurrence  = as.factor(occurrence)
                             
)


```

### Creating recipe to impute missing data 


```{r}
# Generating a recipe to impute missing data 

rcp1 <- recipe(occurrence~.,train_data) %>% step_modeimpute(all_nominal()) %>% step_meanimpute(all_numeric()) %>% step_range(all_numeric()) %>% prep() 

rcp1

# Apply the recipe to the training and test data

train_data1 <- bake(rcp1,train_data)

test_data1= bake(rcp1,test_data)

# Converting factor variables to dummy variable for each level

train_data2 <- fastDummies::dummy_cols(train_data1, remove_first_dummy = TRUE) 

test_data2 <- fastDummies::dummy_cols(test_data1, remove_first_dummy = TRUE)

#Dropping Factor variables except for the dependent variable

train_data2= train_data2 %>% select(-REGION,-SEX,-HISPAN_I,-RACERPI2,-R_MARITL,-DOINGLWA,-HYPYR1,-CHLYR,-MIEV,-STREV,-COPDEV,-CANEV,-PREGEVER,-DIBEV1,-EPILEP1,-SMKNOW,-ALC12MWK,-AHCAFYR2,-AHCSYR1,-AINDPRCH,-MBO_MAN1,-MBO_MND1,-YTQU_YG1,-ERNYR_P,-PLBORN,-PHSTAT,-occurrence_1)

test_data2= test_data2 %>% select(-REGION,-SEX,-HISPAN_I,-RACERPI2,-R_MARITL,-DOINGLWA,-HYPYR1,-CHLYR,-MIEV,-STREV,-COPDEV,-CANEV,-PREGEVER,-DIBEV1,-EPILEP1,-SMKNOW,-ALC12MWK,-AHCAFYR2,-AHCSYR1,-AINDPRCH,-MBO_MAN1,-MBO_MND1,-YTQU_YG1,-ERNYR_P,-PLBORN,-PHSTAT,-occurrence_1)
```

```{r}
#Changing names of the levels on dependent variable occurrence
levels(train_data2$occurrence)

levels(train_data2$occurrence) <- c("Not_occur", "occur") #occur = depresssion/anxiety occured, Not_occur = depresssion/anxiety did not occur

test_data2=test_data2 %>% select(-DOINGLWA_NA,-AHCAFYR2_NA,-SMKNOW_NA)

```


## Glimpsing data

```{r}
glimpse(train_data2) #gives a glimpse of the dataset train_data2
glimpse(test_data2) #gives a glimpse of the dataset test_data2

```


## Creating Folds

```{r}
set.seed(1988) # set a seed for replication purposes 


folds <- createFolds(train_data2$occurrence , k = 10) # Partition the data into 10 equal folds

sapply(folds,length) #displays the folds with their length. 

control_conditions <- 
  trainControl(method='cv', # K-fold cross validation
               summaryFunction = twoClassSummary, # Need this b/c it's a classification problem
               classProbs = TRUE, # Need this b/c it's a classification problem
               index = folds # The indices for our folds (so they are always the same)
  )
```

## Running a Logistic Regression on occurrence  with all other variables in train_data2

```{r}


mod_logit <-
  train(occurrence ~ ., 
        data=train_data2, # Training data 
        method = "glm", # logit function
        metric = "ROC", # area under the curve
        trControl = control_conditions
  )
?make.names
  
```

### Printing the results of the linear model
```{r}
mod_logit #prints the result of the linear model
```
  
## K-Nearest Neighbors 

```{r} 
#Runs a KNN Model for classification

mod_knn <-
  train(coverage ~ ., # Equation (outcome and everything else)
        data=train_data3, # Training data 
        method = "knn", # K-Nearest Neighbors Algorithm
        metric = "ROC", # area under the curve
        trControl = control_conditions
  
  )
```
  
### Printing the results of the KNN model  
```{r}
mod_knn #Prints the results of the KNN model
```

### Plotting the KNN model

```{r}
plot(mod_knn) #plots the knn model
```

## Tuning KNN

```{r}
#Setting tuning parameters
knn_tune = expand.grid(k = c(1,3,10,50)) #
knn_tune
```

```{r}
#Running KNN with tuning parameters

mod_knn2 <-
  train(occurrence ~ ., # Equation (outcome and everything else)
        data=train_data2, # Training data 
        method = "knn", # K-Nearest Neighbors Algorithm
        metric = "ROC", # area under the curve
        tuneGrid = knn_tune, # add the tuning parameters here 
        trControl = control_conditions
  )
```

```{r}
#Display results of KNN tunning model

mod_knn2
```


```{r}
#plots the tunning knn model
plot(mod_knn2)

```


## Classification and Regression Trees (CART)

```{r}
mod_cart <-
  train(occurrence ~ ., # Equation (outcome and everything else)
        data=train_data2, # Training data 
        method = "rpart", # Classification Tree
        metric = "ROC", # area under the curve
        trControl = control_conditions
  )

```

### Printing the results of the CART 

```{r}
mod_cart #prints the results of the CART 
```

### Graphing the CART

```{r}
plot(mod_cart) #Graphs the CART
```


### Visualizing the Tree

```{r}
fancyRpartPlot(mod_cart$finalModel) #Plots the Tree
```

### Let's try to model a tree with depth 0.006

```{r}
tune_cart <- expand.grid(cp = c(0.03)) # Complexity Parameter (how "deep" our trees should grow)
mod_cart2 <-
  train(occurence ~ ., # Equation (outcome and everything else)
        data=train_data2, # Training data 
        method = "rpart", # Classification Tree
        metric = "RMSE",     # mean squared error
        tuneGrid = tune_cart, # Tuning parameters
        trControl = control_conditions  )
```

### Printing the results of the tuned Tree

```{r}

print(tune_cart)

```

### Random Forest

```{r}

mod_rf <-
  train(occurrence ~ ., # Equation (outcome and everything else)
        data=train_data2, # Training data 
        method = "ranger", # random forest (ranger is much faster than rf)
        metric = "ROC", # area under the curve
        trControl = control_conditions
  
  )
```
1


### Printing the Random Forest Model
```{r}
mod_rf
```

### Ploting the Random Forest Model

```{r}
plot(mod_rf)
```

## Running a SVM Model

```{r}
#Runs SVM liner model

mod_svm_linear <-
  train(occurrence ~ .,
        data=train_data2, # Training data 
         method = "svmLinear", # SVM with a polynomial Kernel
        metric = "ROC", # area under the curve
        tuneGrid = expand.grid(C = c(.5,1)), # Add two tuning parameters
        trControl = control_conditions
  )

```

```{r}
#Displays SVM model results
mod_svm_linear
```

## Running a Polynomial Boundary

```{r}
#Runs SVM liner model
mod_svm_poly <-
  train(occurrence ~.,
        data=train_data2, # Training data 
         method = "svmPoly", # SVM with a polynomial Kernel
        metric = "ROC", # area under the curve
        trControl = control_conditions
  )
mod_svm_poly #Displays results from Polynomial Boundry
```

## Runnig Radial Boundary 

```{r}
#Runs the SVM Radial Boundary Model
mod_svm_radial <-
  train(occurrence ~ .,
        data=train_data2, # Training data 
         method = "svmPoly", # SVM with a Radial Kernel
        metric = "ROC", # area under the curve
        trControl = control_conditions
  )

mod_svm_radial #Displays results from Radial Boundry
```


## Model Comparison

```{r}
# Organize all model imputs as a list.
mod_list <-
  list(
    mod_logit,
    knn1 = mod_knn,
    knn2 = mod_knn2,
    cart1 = mod_cart,
    rf = mod_rf,
    svm_linear = mod_svm_linear,
    svm_poly = mod_svm_poly,  
    svm_radial = mod_svm_radial 
  )

# Generate Plot to compare output. 
dotplot(resamples(mod_list))
```

### Examine the error across each of the five models.
```{r,fig.width=10,fig.height=4}
#plots a each model and it's ROC

dotplot(resamples(mod_list),metric = "ROC")
```


## Testing the Predictive Accuracy of the Random Forest Model 


```{r}
pred <- predict(mod_rf,newdata = test_data2)

summarypred=summary(pred)

summarypred

skimr::skim(pred)

``` 
