---
title: "PPOL670 FINAL PROJECT"
output: 
 pdf_document:
 toc: true
fontsize: 12pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

# **Predicting Depression and Anxiety using Supervised Learning Models**
### *Jagir Pipalia*

*******

## **Introduction**

In 2015, over 43 million adults had a mental illness, and nearly 10 million had a serious mental illness, such as depression, bipolar disorder, or schizophrenia . People with mental health conditions often have chronic medical conditions, significant health care services utilization, and barriers to employment, and are frequently involved with the criminal justice system.[^fn1] 

In this paper I used data from the National Health Interview Survey to generate a supervised learning model that predicts the occurrence of depression, anxiety, or emotional problems among respondents.

I utilize the National Health Interview Survey to identify datasets that contain variables that are relevant in predicting the occurence of depression and anxiety. I explored the datasets to determine the variables that may predict the occurence of depression and anxiety. I import the variables in R to create a R dataset. I than take a random sample from the dataset for building a supervised learning models to predict the occurence of depression and anxiety. I compared the models to find the model with the best predictive ability.    

*******

## **Problem Statement and Background**

Depression is among the leading causes of disability in industrialized countries.To effectively target interventions for patients at risk for a worse long-term clinical outcome, there is a need to identify predictors of chronicity and remission at an early stage. This could allow a quicker escalation of treatment for patients with a low long-term chance of recovery, thus potentially avoiding initial treatment resistance.[^fn2]  

Neuroscientists and clinicians around the world are using machine learning to develop treatment plans for patients and to identify some of the key markers for mental health disorders before they may set in. One of the benefits is that machine learning helps clinicians predict who may be at risk of a particular disorder.There is so much data available that we are now able to compile data for mental health professionals so they may do their job better. What makes machine learning so helpful today is that in the past, understanding of diagnoses were based off group averages and statistics over populations. Machine learning gives clinicians the opportunity to personalize.[^fn3]

David and colleagues use a layered, hierarchical model for translating raw sensor data into markers of behaviors and states related to mental health.[^fn4] 

Through this project, I aim to create a supervised learning model that can accurately predict if a person is at risk of experiencing depression or anxiety. I take health variables on several diseases such as Hypertension, Cholestrol, COPD, Diebetes, etc and combine it with a person's age, weight, race, and region they combine along with other variables to predict the occurence of depression or anxiety. 

*******

## **Data**

The data for the project comes from The National Health Interview Survey (NHIS). NHIS has monitored the health of the nation since 1957. NHIS data on a broad range of health topics, including mental health conditions, are collected through personal household interviews. The U.S. Census Bureau has been the data collection agent for the National Health Interview Survey. Survey results have been instrumental in providing data to track health status, health care access, and progress toward achieving national health objectives. 

The unit of observation is survey participant. Each participant is an observation. I selected 30 variables from the NHIS datasets that seemed relevant to the question. There were 104874 observations in total. The variable of interest was AFLHCA17 which is marker for if depression, anxiety, or emotional problems caused difficulty with activity.

Most of the variables are categorical values. Some variables appear to be continues variables. There is a lot of missing data, so I will impute the missing data.

```{r include=FALSE}
require(tidyverse) #loads the package tidyverse
require(caret) #loads the package caret
require(lubridate) #loads the package lubridate
require(ggplot2) #loads the package ggplot2
require(recipes) #loads the package recipes
require(rattle) # For nice tree plots
require(ranger) # loads the package ranger
require(skimr) # loads the package skimr
require(ggthemes) #loads the package ggthemes
require(mice) #loads the package mice
require(fastDummies) #For quickly creating dummy variables
require(dendroTools) #For quickly rounding numbers
require(kernlab) #For Running SVM models
require(stargazer) #loads stargazer package

```



```{r include=FALSE}

sample_adult_raw = read_csv("Raw Data/samadult.csv") #imports data into the R memory 
person_raw = read_csv("Raw Data/personsx.csv") #imports data into the R memory 


```


```{r  include=FALSE}
#creating datasets by selecting variables of interest from raw data

dataset_adult = sample_adult_raw %>% select(AFLHCA17,RECTYPE,REGION,SEX,HISPAN_I,RACERPI2,AGE_P,R_MARITL,DOINGLWA,HYPYR1,CHLYR,MIEV,STREV,COPDEV,CANEV,PREGEVER,DIBEV1,EPILEP1,SMKNOW,ALC12MWK,AHCAFYR2,AHCSYR1,AINDPRCH,MBO_MAN1,MBO_MND1,YTQU_YG1,AHEIGHT,AWEIGHTP)

dataset_person = person_raw %>% select(RECTYPE,ERNYR_P,PLBORN,PHSTAT)

```



```{r  include=FALSE}
#merging the two datasets
final_dataset_raw = full_join(dataset_person,dataset_adult,by="RECTYPE")
sapply(colnames(final_dataset_raw), function(x) class(final_dataset_raw[[x]])) #Gives us the class of each variable

```

```{r  include=FALSE}
#Dropiing Variable RECTYPE as it is an record identifier and was used to merge datasets
final_dataset_raw=final_dataset_raw  %>% select(-RECTYPE)
```


```{r  include=FALSE}
final_dataset_raw=mutate(final_dataset_raw,
       occurrence = AFLHCA17 == '1',
       occurrence = as.numeric(occurrence)) #Creating a new variable 'occurrence' based on AFLHCA17
final_dataset_raw=final_dataset_raw  %>% select(-AFLHCA17) #Dropping the variable AFLHCA17 as we longer need it

```


```{r  include=FALSE}
final_dataset=final_dataset_raw %>% sample_n(10000) # Only taking a random sample of the data so the models run quicker
head(final_dataset) # Peek at the data just to make sure everything was read in correctly. 


```


```{r include=FALSE}
# Gives a summary of all the variables in the dataset
glimpse(final_dataset)

```


```{r  include=FALSE}
## 75% of the sample size
smp_size <- floor(0.75 * nrow(final_dataset))

## Creating partion
set.seed(123)
train_ind <- sample(seq_len(nrow(final_dataset)), size = smp_size)

train_data <- final_dataset[train_ind, ] #creates training dataset
test_data <- final_dataset[-train_ind, ] #creates test dataset
```

```{r include=FALSE}
skimr::skim(train_data) #Gives a summary statics of all the variables in train_data
```

### **Data Wrangling**

I first imported the datasets 'dataset_adult' and 'dataset_person'. I then selected the following variables using the tidyverse package:

AFLHCA17,RECTYPE,REGION,SEX,HISPAN_I,RACERPI2,AGE_P,R_MARITL,DOINGLWA,HYPYR1,CHLYR,MIEV,STREV,COPDEV,CANEV,PREGEVER,DIBEV1,EPILEP1,SMKNOW,ALC12MWK,AHCAFYR2,AHCSYR1,AINDPRCH,MBO_MAN1,MBO_MND1,YTQU_YG1,AHEIGHT,AWEIGHTP,RECTYPE,ERNYR_P,PLBORN,and PHSTAT

I then merged the two datasets using 'full_join' function to form the 'final_dataset_raw'.In order to minimize the time to run the machine learning models, I took a random sample of 10,000 observations from 'final_dataset_raw'.The sample was named 'final_dataset'. I divided the 'final_dataset' into training data and test data. Training data has 75% of the observations while test data has the remaining 25% of the data. 

### **Vizualizations for Independent Variables**

Following are vizualizations for some of the indepedent variables. As we can see from the density graph for Age variable. Majority of participants seem to be above 50 years of age. 
 
```{r,fig.width=6,fig.height=4, echo=FALSE}
# Creating a density graph for Age
train_data %>% 
  select(AGE_P) %>% 
  gather(var,val) %>% 
  ggplot(aes(val,group=var)) +
  geom_density(bins = 68, fill = "azure") + ggtitle("Distribution of observations by Age")+ # for the main title
xlab("Age")+ # for the x axis label
ylab("Frequency")+# for the y axis label
  theme_tufte()


```

From the bar graph below for Height variable, we can see that there are some outliers.

```{r,fig.width=6,fig.height=4, echo=FALSE}
# Creating a histogram for Height
train_data %>% 
  select(AHEIGHT) %>% 
  gather(var,val) %>% 
  ggplot(aes(val,group=var)) +
  geom_histogram(bins =30, fill = "darkolivegreen") + ggtitle("Distribution of observations by Height") + # for the main title
xlab("Height")+ # for the x axis label
ylab("Frequency")+ # for the y axis label  
  theme_stata()


```

Similarly, as evident in the histogram below, the Weight variable also has some outliers:

```{r,fig.width=6,fig.height=4, echo=FALSE}
# Creating a histogram for Weight
train_data %>% 
  select(AWEIGHTP) %>% 
  gather(var,val) %>% 
  ggplot(aes(val,group=var)) +
  geom_histogram(bins =30, fill = "cornflowerblue") + ggtitle("Distribution of observations by Weight")+ # for the main title
xlab("Weight")+ # for the x axis label
ylab("Frequency")+# for the y axis label
  theme_stata()


```


Following is a bar graph for Region variable. The highest number of observations were from the South region.

```{r,fig.width=6,fig.height=4, echo=FALSE}
# Creating a bar graph for Region variable
train_data %>% 
  mutate(REGION_bins = case_when(
    REGION == 1 ~ "Northeast",
    REGION == 2 ~ "Midwest",
    REGION == 3 ~ "South",
    REGION == 4 ~ "West",
  )) %>% 
  mutate(REGION_bins = fct_infreq(REGION_bins)) %>% 
  ggplot(aes(REGION_bins)) + geom_bar(fill="steelblue") + ggtitle("Distribution of observations by Region")+ # for the main title
xlab("Region")+ # for the x axis label
ylab("Frequency")+
  theme_economist()
  
```

Following is a bar graph for Sex variable. There are more females in the dataset than males.

```{r,fig.width=6,fig.height=4, echo=FALSE}
# Creating a bar graph for Sex variable
train_data %>% 
  mutate(SEX_bins = case_when(
    SEX == 1 ~ "Male",
    SEX == 2 ~ "Female",
    
  )) %>% 
  mutate(SEX_bins = fct_infreq(SEX_bins)) %>% 
  ggplot(aes(SEX_bins)) + geom_bar(fill="darkred") + ggtitle("Distribution of observations by Sex")+ # for the main title
xlab("Sex")+ # for the x axis label
ylab("Frequency")+
  theme_economist()
  
```

Following is a bar graph for Race variable. The highest number of observations are White.

```{r,fig.width=6,fig.height=4, echo=FALSE}
# Creating a bar graph for Race Variable
train_data %>% 
  mutate(RACERPI2_bins = case_when(
    RACERPI2 == 1 ~ "White",
    RACERPI2 == 2 ~ "African American",
    RACERPI2 == 3 ~ "AIAN",
    RACERPI2 == 4 ~ "Asian",
    RACERPI2 == 5 ~ "Unknown",
    RACERPI2 == 6 ~ "Multiple Race",
    
  )) %>% 
  mutate(RACERPI2_bins = fct_infreq(RACERPI2_bins)) %>% 
  ggplot(aes(RACERPI2_bins)) + geom_bar(fill="darkolivegreen") + ggtitle("Distribution of observations by Race")+ # for the main title
xlab("Race")+ # for the x axis label
ylab("Frequency")+
  theme_economist()
  
```

Following is a bar graph for Earnings variable. There are significant number of observations that earn more than $75,000.

```{r,fig.width=6,fig.height=4, echo=FALSE}
# Creating a bar graph for Earnings
train_data %>% 
  mutate(ERNYR_P_bins = case_when(
    ERNYR_P == 01 ~ "0-5k",
    ERNYR_P == 02 ~ "5k-10k",
    ERNYR_P == 03 ~ "10k-15k",
ERNYR_P == 04 ~ "15k-20k",
ERNYR_P == 05 ~ "20k-25k",
ERNYR_P == 06 ~ "25k-35k",
ERNYR_P == 07 ~ "35k-45k",
ERNYR_P == 08 ~ "45k-55k",
ERNYR_P == 09 ~ "55k-65k",
ERNYR_P == 10 ~ "65k-75k",
ERNYR_P == 11 ~ "75k+",
ERNYR_P == 97 ~ "Refused",
ERNYR_P == 98 ~ "Not Certain",
ERNYR_P == 99 ~ "Don't Know"
,
    
    
    
    
  )) %>% 
  mutate(ERNYR_P_bins = fct_infreq(ERNYR_P_bins)) %>% 
  ggplot(aes(ERNYR_P_bins)) + geom_bar(fill="chocolate4") + ggtitle("Distribution of observations by Earnings")+ # for the main title
xlab("Total Earnings")+ # for the x axis label
ylab("Frequency")+
  theme_economist() +coord_flip()
  
```



```{r echo=FALSE}
#Changing class of categorical variables to factor
train_data=train_data %>% mutate(ERNYR_P = as.factor(ERNYR_P),
                             PLBORN = as.factor(PLBORN),
                             PHSTAT = as.factor(PHSTAT),
                             REGION = as.factor(REGION),
                             SEX = as.factor(SEX),
                             HISPAN_I = as.factor(HISPAN_I),
                             RACERPI2 = as.factor(RACERPI2),
                             R_MARITL = as.factor(R_MARITL),
                             DOINGLWA = as.factor(DOINGLWA),
                             HYPYR1 = as.factor(HYPYR1),
                             CHLYR = as.factor(CHLYR),
                             MIEV = as.factor(MIEV),
                             STREV = as.factor(STREV),
                             COPDEV = as.factor(COPDEV),
                             CANEV = as.factor(CANEV),
                             PREGEVER = as.factor(PREGEVER),
                             DIBEV1 = as.factor(DIBEV1),
                             EPILEP1 = as.factor(EPILEP1),
                             SMKNOW = as.factor(SMKNOW),
                             ALC12MWK = as.factor(ALC12MWK),
                             AHCAFYR2 = as.factor(AHCAFYR2),
                             AHCSYR1 = as.factor(AHCSYR1),
                             AINDPRCH = as.factor(AINDPRCH),
                             MBO_MAN1 = as.factor(MBO_MAN1),
                             MBO_MND1 = as.factor(MBO_MND1),
                             YTQU_YG1 = as.factor(YTQU_YG1),
                             occurrence  = as.factor(occurrence)
                             
)


```

### **Pre-Processing**

As it can be seen from the above graphs, the sample has a high number of missing values. I generated a recipe using the recipes package to impute the missing data and scale the data. I also created a dummy variable for each level of the factor variables for each factor variable. 

After processing the data using the recipe generated, the data was ready to be used in supervised learning models.


```{r  include=FALSE}
# Generating a recipe to impute missing data 

rcp1 <- recipe(occurrence~.,train_data) %>% step_modeimpute(all_nominal()) %>% step_meanimpute(all_numeric()) %>% step_range(all_numeric()) %>% prep() 

rcp1

# Apply the recipe to the training and test data

train_data1 <- bake(rcp1,train_data)

test_data1= bake(rcp1,test_data)

# Converting factor variables to dummy variable for each level

train_data2 <- fastDummies::dummy_cols(train_data1, remove_first_dummy = TRUE) 

test_data2 <- fastDummies::dummy_cols(test_data1, remove_first_dummy = TRUE)

#Dropping Factor variables except for the dependent variable

train_data2= train_data2 %>% select(-REGION,-SEX,-HISPAN_I,-RACERPI2,-R_MARITL,-DOINGLWA,-HYPYR1,-CHLYR,-MIEV,-STREV,-COPDEV,-CANEV,-PREGEVER,-DIBEV1,-EPILEP1,-SMKNOW,-ALC12MWK,-AHCAFYR2,-AHCSYR1,-AINDPRCH,-MBO_MAN1,-MBO_MND1,-YTQU_YG1,-ERNYR_P,-PLBORN,-PHSTAT,-occurrence_1)

test_data2= test_data2 %>% select(-REGION,-SEX,-HISPAN_I,-RACERPI2,-R_MARITL,-DOINGLWA,-HYPYR1,-CHLYR,-MIEV,-STREV,-COPDEV,-CANEV,-PREGEVER,-DIBEV1,-EPILEP1,-SMKNOW,-ALC12MWK,-AHCAFYR2,-AHCSYR1,-AINDPRCH,-MBO_MAN1,-MBO_MND1,-YTQU_YG1,-ERNYR_P,-PLBORN,-PHSTAT,-occurrence_1)
```

```{r  include=FALSE}
#Changing names of the levels on dependent variable occurrence
levels(train_data2$occurrence)

levels(train_data2$occurrence) <- c("Not_occur", "occur") #occur = depresssion/anxiety occured, Not_occur = depresssion/anxiety did not occur

```




```{r  include=FALSE}
glimpse(train_data2) #gives a glimpse of the dataset train_data2
glimpse(test_data2) #gives a glimpse of the dataset test_data2

```
*******

## **Analysis**

I started by setting cross-validation settings. I used k-fold validation. I used 10 folds as the training data contains 7500 observations. Each fold was about 750 observations. The dependent variable, occurence, in my analysis is a dummy variable which takes the value 1 when the observation experienced depression or anxiety. It takes the value 0 when the observation did not report experiencing depression or anxiety. 

I began with my first model which was Logistic Regression, I set the metric to be 'ROC' as the dependent variable is a dummy variable. As we can see from the output, the ROC from the Logistic Regression is close 0.5, which is almost as good as a coin-toss. Therefore, I decided to use KNN model next.


```{r  include=FALSE}
set.seed(1988) # set a seed for replication purposes 


folds <- createFolds(train_data2$occurrence , k = 10) # Partition the data into 10 equal folds

sapply(folds,length) #displays the folds with their length. 

control_conditions <- 
  trainControl(method='cv', # K-fold cross validation
               summaryFunction = twoClassSummary, # Need this b/c it's a classification problem
               classProbs = TRUE, # Need this b/c it's a classification problem
               index = folds # The indices for our folds (so they are always the same)
  )
```


```{r  include=FALSE}


mod_logit <-
  train(occurrence ~ ., 
        data=train_data2, # Training data 
        method = "glm", # logit function
        metric = "ROC", # area under the curve
        trControl = control_conditions
  )

```

```{r echo=FALSE}
mod_logit #prints the result of the linear model
```
  

```{r  include=FALSE} 
#Runs a KNN Model for classification

mod_knn <-
  train(occurrence ~ ., # Equation (outcome and everything else)
        data=train_data2, # Training data 
        method = "knn", # K-Nearest Neighbors Algorithm
        metric = "ROC", # area under the curve
        trControl = control_conditions
  
  )
```


  
```{r  include=FALSE}
mod_knn #Prints the results of the KNN model
```

As we can see from the output below, the KNN model with k=9, perfomed better than other knn models and also better than Logistic Regression.The ROC was close to 0.6. In order to see if the ROC would increase with a higher k, I run a tuned KNN model with the highest k equal to 50.

```{r echo=FALSE}
plot(mod_knn) #plots the knn model
```


```{r  include=FALSE}
#Setting tuning parameters
knn_tune = expand.grid(k = c(1,3,10,50)) #
knn_tune
```

```{r  include=FALSE}
#Running KNN with tuning parameters

mod_knn2 <-
  train(occurrence ~ ., # Equation (outcome and everything else)
        data=train_data2, # Training data 
        method = "knn", # K-Nearest Neighbors Algorithm
        metric = "ROC", # area under the curve
        tuneGrid = knn_tune, # add the tuning parameters here 
        trControl = control_conditions
  )
```

```{r  include=FALSE}
#Display results of KNN tunning model

mod_knn2
```

As we can see, the ROC continues to improve with higher ks and reaches close to 0.75 when k=50. I run the CART and Random Forest models next to see how well they do in predicting occurrence.
```{r echo=FALSE}
#plots the tunning knn model
plot(mod_knn2)

```



```{r  include=FALSE}
mod_cart <-
  train(occurrence ~ ., # Equation (outcome and everything else)
        data=train_data2, # Training data 
        method = "rpart", # Classification Tree
        metric = "ROC", # area under the curve
        trControl = control_conditions
  )

```


```{r  include=FALSE}
mod_cart #prints the results of the CART 
```


```{r  include=FALSE}

mod_rf <-
  train(occurrence ~ ., # Equation (outcome and everything else)
        data=train_data2, # Training data 
        method = "ranger", # random forest (ranger is much faster than rf)
        metric = "ROC", # area under the curve
        trControl = control_conditions
  
  )
```



```{r  include=FALSE}
mod_rf
```

The CART model had an ROC of 0.5. The random forest model had a much higher ROC of 0.94 with 2 randomly selected variables and using gini splitting rule. I then run SVM models followed by a Polynomial Boundary and Radial boundary model. The SVM Model has a ROC close to 0.7 while the Polynomial Boundary and Radial Boundary models have ROC close to 0.81.
 

```{r echo=FALSE}
plot(mod_rf)
```


```{r  include=FALSE}
#Runs SVM liner model

mod_svm_linear <-
  train(occurrence ~ .,
        data=train_data2, # Training data 
         method = "svmLinear", # SVM with a polynomial Kernel
        metric = "ROC", # area under the curve
        tuneGrid = expand.grid(C = c(.5,1)), # Add two tuning parameters
        trControl = control_conditions
  )

```

```{r  include=FALSE}
#Displays SVM model results
mod_svm_linear
```


```{r  include=FALSE}
#Runs SVM liner model
mod_svm_poly <-
  train(occurrence ~.,
        data=train_data2, # Training data 
         method = "svmPoly", # SVM with a polynomial Kernel
        metric = "ROC", # area under the curve
        trControl = control_conditions
  )
mod_svm_poly #Displays results from Polynomial Boundry
```


```{r  include=FALSE}
#Runs the SVM Radial Boundary Model
mod_svm_radial <-
  train(occurrence ~ .,
        data=train_data2, # Training data 
         method = "svmPoly", # SVM with a Radial Kernel
        metric = "ROC", # area under the curve
        trControl = control_conditions
  )

mod_svm_radial #Displays results from Radial Boundry
```
*******

## **Results**

We ran Logistic Regression, KNN, KNN with tunning parameters, CART, Random Forest, SVM Linear, SVM Polynomial Boundary, and SVM Radial Boundary models. Following dotplot compares the performace of models with each other. As we can observe the random forest model performed the best in terms of ROC. The SVM models also performed very well.

```{r echo=FALSE}
# Organize all model imputs as a list.
mod_list <-
  list(
    mod_logit,
    knn1 = mod_knn,
    knn2 = mod_knn2,
    cart1 = mod_cart,
    rf = mod_rf,
    svm_linear = mod_svm_linear,
    svm_poly = mod_svm_poly,  
    svm_radial = mod_svm_radial 
  )

# Generate Plot to compare output. 
dotplot(resamples(mod_list))
```
The following plot gives a closer look at comparision of ROC accross models

```{r,fig.width=10,fig.height=4, echo=FALSE}
#plots a each model and it's ROC

dotplot(resamples(mod_list),metric = "ROC")
```


Since, the random forest model performed the best, we will use to predict the occurrence of dpression or anxiety in the test dataset.  As we can see from the results below, the model predicts 0 occurence of depression or anxiety in tre test dataset. 

```{r echo=FALSE}
pred <- predict(mod_rf,newdata = test_data2)

summarypred=summary(pred)

summarypred


``` 
*******

## **Discussion**

Although the Random model and SVM models had a high ROC, we did not see a good prediction for occurrence from the test dataset. I think the project was partially successful as it was able to create a good predictive model for occurrence of depression or anxiety as it had high ROC but it was not good in actually predicting the occurrence from the test dataset. 

I considered using a larger sample and variables with more variation, but due to time and computational constraints, I was not able do it. In the future, I plan to use a bigger sample along with more suitable variables which have a lower level of missing vlaues. 


[^fn1]: KFF. (2019). Facilitating Access to Mental Health Services: A Look at Medicaid, Private Insurance, and the Uninsured. [online] Available at: https://www.kff.org/medicaid/fact-sheet/facilitating-access-to-mental-health-services-a-look-at-medicaid-private-insurance-and-the-uninsured/:~:targetText=The%20Medicaid%20program%20covers%20many,,%20counseling,%20and%20prescription%20medications.&targetText=The%20Medicaid%20expansion%20has%20enabled,obtain%20coverage%20and%20access%20treatment. [Accessed 12 Nov. 2019].

[^fn2]: Dinga, R., Marquand, A. F., Veltman, D. J., Beekman, A., Schoevers, R. A., van Hemert, A. M., … Schmaal, L. (2018). Predicting the naturalistic course of depression from a wide range of clinical, psychological, and biological data: a machine learning approach. Translational psychiatry, 8(1), 241. doi:10.1038/s41398-018-0289-1

[^fn3]: Abbas, N. (2019, August 29). Machine Learning and Mental Health Can Big Data help the Mental Health field? Retrieved December 13, 2019, from https://towardsdatascience.com/machine-learning-and-mental-health-7981a6001bd5.

[^fn4]: Mohr, D. C., Zhang, M., & Schueller, S. M. (2017). Personal Sensing: Understanding Mental Health Using Ubiquitous Sensors and Machine Learning. Annual Review of Clinical Psychology, 13(1), 23–47. doi: 10.1146/annurev-clinpsy-032816-044949